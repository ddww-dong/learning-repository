{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建神经网络",
   "id": "3cdd40962dcba517"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T06:30:18.145857Z",
     "start_time": "2025-02-23T06:30:10.658542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from keras.src.utils.module_utils import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 获取训练设备\n",
   "id": "531f4f7123c48cc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:30:19.970504Z",
     "start_time": "2025-02-23T06:30:19.958385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ],
   "id": "fc4744921e3c40d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 定义类",
   "id": "a23164784ee90f5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们通过继承 nn.Module 来定义我们的神经网络，并在 __init__ 中初始化神经网络层。每个 nn.Module 子类都在 forward 方法中实现对输入数据的操作。",
   "id": "c0f0dd186c96b2b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:35:41.247770Z",
     "start_time": "2025-02-23T06:35:41.235213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ],
   "id": "1414c6aa3ca932c4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:35:41.981001Z",
     "start_time": "2025-02-23T06:35:41.953891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "id": "86ef92840379f35e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:35:55.380154Z",
     "start_time": "2025-02-23T06:35:55.348937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ],
   "id": "21f35cad74fa8da8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 其中，从构建模型层开始",
   "id": "a70b08495858bbd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "首先模拟生成图片输入",
   "id": "14e2671f13f69e4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:49:39.693557Z",
     "start_time": "2025-02-23T06:49:39.678023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ],
   "id": "5dd676d4ad087b79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### nn.Flatten",
   "id": "5b550fd58bc329d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:50:01.065725Z",
     "start_time": "2025-02-23T06:50:01.059721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ],
   "id": "e75b0ca19ee9135b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### nn.Linear\n",
   "id": "56b1c4f5051e04bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:57:33.012821Z",
     "start_time": "2025-02-23T06:57:32.957831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ],
   "id": "354cd9b6f16fe670",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### nn.ReLU",
   "id": "447f2fe9fff13547"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:58:06.154543Z",
     "start_time": "2025-02-23T06:58:06.144024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ],
   "id": "8c542211ed4bd9d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.2151, -0.4946,  0.2322, -0.3633, -0.1978, -0.1958,  0.3194, -0.1729,\n",
      "          0.2270,  0.1978, -0.1490,  0.1652,  0.0130,  0.2478,  0.6004,  0.5054,\n",
      "          0.1026, -0.4824, -0.2080, -0.4235],\n",
      "        [-0.2996, -0.2095,  0.3009, -0.0009, -0.2728, -0.2079,  0.1153, -0.0926,\n",
      "          0.3164,  0.1010, -0.1627,  0.1365, -0.3530,  0.5112, -0.0093, -0.0232,\n",
      "          0.2357, -0.6794,  0.0710, -0.3191],\n",
      "        [-0.2474, -0.2013,  0.1729, -0.0458,  0.0051, -0.2627,  0.0679,  0.0391,\n",
      "          0.2579,  0.2285, -0.6275,  0.2015, -0.1899,  0.6176,  0.3250,  0.4509,\n",
      "          0.0440, -0.3656, -0.0850, -0.3074]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.2322, 0.0000, 0.0000, 0.0000, 0.3194, 0.0000, 0.2270,\n",
      "         0.1978, 0.0000, 0.1652, 0.0130, 0.2478, 0.6004, 0.5054, 0.1026, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3009, 0.0000, 0.0000, 0.0000, 0.1153, 0.0000, 0.3164,\n",
      "         0.1010, 0.0000, 0.1365, 0.0000, 0.5112, 0.0000, 0.0000, 0.2357, 0.0000,\n",
      "         0.0710, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1729, 0.0000, 0.0051, 0.0000, 0.0679, 0.0391, 0.2579,\n",
      "         0.2285, 0.0000, 0.2015, 0.0000, 0.6176, 0.3250, 0.4509, 0.0440, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### nn.Sequential\n",
    "模块的有序容器，数据通过组合的顺序通过所有模块，可用于组合神经网络"
   ],
   "id": "8b7291696d1bd094"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T07:00:30.290805Z",
     "start_time": "2025-02-23T07:00:30.271019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seq_models = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_models(input_image)\n"
   ],
   "id": "fdd671dcdd6c1cab",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### nn.Softmax\n",
   "id": "142c823b0948a5ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T07:01:38.585714Z",
     "start_time": "2025-02-23T07:01:38.557936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probab = softmax(logits)"
   ],
   "id": "2a606edd3c240a74",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "神经网络内部的许多层都是参数化的，即具有在训练期间优化的相关权重和偏置。继承 nn.Module 会自动跟踪模型对象内定义的所有字段，并使所有参数都可以使用模型的 parameters() 或 named_parameters() 方法访问。\n",
    "\n",
    "在此示例中，我们遍历每个参数，并打印其大小和值的预览。"
   ],
   "id": "6690c9a29b468954"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T07:49:45.131186Z",
     "start_time": "2025-02-23T07:49:45.115538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ],
   "id": "6dfe3c41a976c925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0070, -0.0195, -0.0182,  ...,  0.0117, -0.0094,  0.0212],\n",
      "        [ 0.0198,  0.0231, -0.0075,  ...,  0.0279,  0.0352,  0.0136]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0011, 0.0137], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0111,  0.0280,  0.0270,  ...,  0.0202, -0.0033, -0.0283],\n",
      "        [ 0.0247, -0.0397,  0.0411,  ..., -0.0024,  0.0385,  0.0290]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0320,  0.0075], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0377,  0.0122,  0.0251,  ..., -0.0152, -0.0010,  0.0097],\n",
      "        [ 0.0418,  0.0278, -0.0326,  ...,  0.0296, -0.0419,  0.0307]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0337, -0.0155], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch--myenv",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
